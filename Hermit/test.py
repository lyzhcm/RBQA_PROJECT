import streamlit as st
import time
from datetime import datetime
import os
import pandas as pd
import json
from docx import Document
from PyPDF2 import PdfReader
import pptx
import tempfile


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []


# æ–‡ä»¶è§£æå‡½æ•°
def parse_file(file):
    content = ""
    file_type = file.name.split(".")[-1].lower()

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        if file_type == "txt":
            with open(tmp_path, "r", encoding="utf-8") as f:
                content = f.read()

        elif file_type == "pdf":
            reader = PdfReader(tmp_path)
            for page in reader.pages:
                content += page.extract_text() + "\n"

        elif file_type == "docx":
            doc = Document(tmp_path)
            for para in doc.paragraphs:
                content += para.text + "\n"

        elif file_type == "pptx":
            prs = pptx.Presentation(tmp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"

        os.unlink(tmp_path)
        return content

    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None


# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            if file not in st.session_state.uploaded_files:
                with st.spinner(f"è§£ææ–‡ä»¶: {file.name}..."):
                    content = parse_file(file)

                    if content:
                        st.session_state.uploaded_files.append({
                            "name": file.name,
                            "type": file.type,
                            "content": content,
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        })

                        # åˆ†å‰²å†…å®¹ä¸ºçŸ¥è¯†ç‰‡æ®µ
                        chunks = split_into_chunks(content)
                        for i, chunk in enumerate(chunks):
                            st.session_state.knowledge_base.append({
                                "source": file.name,
                                "content": chunk,
                                "type": file.type.split("/")[-1]
                            })

        # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
        st.subheader("å·²ä¸Šä¼ æ–‡æ¡£")
        files_df = pd.DataFrame([
            {
                "æ–‡ä»¶å": f["name"],
                "ç±»å‹": f["type"],
                "ä¸Šä¼ æ—¶é—´": f["upload_time"]
            }
            for f in st.session_state.uploaded_files
        ])
        st.dataframe(files_df)

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹"):
            for item in st.session_state.knowledge_base[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                st.caption(f"æ¥æº: {item['source']} | ç±»å‹: {item['type']}")
                st.text(item["content"][:150] + "..." if len(item["content"]) > 150 else item["content"])
                st.divider()


# å°†æ–‡æ¡£å†…å®¹åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„ç‰‡æ®µ
def split_into_chunks(text, chunk_size=200):
    chunks = []
    words = text.split()

    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i + chunk_size]))

    return chunks


# é—®ç­”ç•Œé¢
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    # æ˜¾ç¤ºå†å²å¯¹è¯
    if st.session_state.conversation:
        st.subheader("å¯¹è¯å†å²")
        for i, message in enumerate(st.session_state.conversation):
            role = "ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·" if i % 2 == 0 else "ğŸ¤– ç³»ç»Ÿ"
            with st.chat_message(name=role):
                st.write(message)
            if i < len(st.session_state.conversation) - 1:
                st.divider()

    # ç”¨æˆ·è¾“å…¥é—®é¢˜
    question = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")

    if question:
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        with st.chat_message(name="ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·"):
            st.write(question)

        # æ¨¡æ‹Ÿæ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹
        with st.spinner("æ­£åœ¨æ€è€ƒå¹¶ç”Ÿæˆç­”æ¡ˆ..."):
            time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

            # ç®€åŒ–çš„æ£€ç´¢è¿‡ç¨‹ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨embeddingç­‰é«˜çº§æŠ€æœ¯ï¼‰
            matching_chunks = []
            for item in st.session_state.knowledge_base:
                if any(word in item['content'] for word in question.split()[:3]):
                    matching_chunks.append(item)

            # æ¨¡æ‹Ÿç”Ÿæˆç­”æ¡ˆ
            if matching_chunks:
                answer = f"æ ¹æ®çŸ¥è¯†åº“ä¸­çš„å†…å®¹ä¸ºæ‚¨è§£ç­”ï¼š\n\n"
                answer += matching_chunks[0]['content'][:300] + "\n\n"
                answer += "ï¼ˆæ­¤ä¸ºæ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…ç³»ç»Ÿä¼šç»“åˆä¸Šä¸‹æ–‡ç”Ÿæˆæ›´è‡ªç„¶çš„å›ç­”ï¼‰"

                # æ˜¾ç¤ºæ¥æº
                answer += f"\n\næ¥æº: {matching_chunks[0]['source']}"
            else:
                answer = "åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜æˆ–ä¸Šä¼ æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚"

            # æ·»åŠ åˆ°å¯¹è¯å†å²
            st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºç­”æ¡ˆ
            with st.chat_message(name="ğŸ¤– ç³»ç»Ÿ"):
                st.write(answer)


# ä¸»ç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide"
    )

    st.title("åŸºäºRAGçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.caption("æ”¯æŒæ–‡æ¡£å¤„ç†ä¸æ™ºèƒ½é—®ç­”åŠŸèƒ½ | é¡¹ç›®å®ç°æ–¹æ¡ˆ")

    init_session()

    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header("å¯¼èˆª")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"])
        st.divider()

        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        1. çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†
        2. æ–‡æ¡£è§£æä¸å¤„ç†
        3. è‡ªç„¶è¯­è¨€é—®ç­”
        4. å¯¹è¯ä¸Šä¸‹æ–‡è®°å½•
        """)

    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()

    # è°ƒè¯•ä¿¡æ¯
    with st.expander("è°ƒè¯•ä¿¡æ¯"):
        st.json({
            "æ–‡ä»¶ä¸Šä¼ æ•°": len(st.session_state.uploaded_files),
            "çŸ¥è¯†ç‰‡æ®µæ•°": len(st.session_state.knowledge_base),
            "å¯¹è¯è½®æ¬¡": len(st.session_state.conversation) // 2
        })


if __name__ == "__main__":
    main()