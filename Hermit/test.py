import streamlit as st
import time
from datetime import datetime
import os
import pandas as pd
import json
from docx import Document
from PyPDF2 import PdfReader
import pptx
import tempfile
import hashlib


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []


# ç”Ÿæˆæ–‡ä»¶å”¯ä¸€ID
def generate_file_id(file_content):
    return hashlib.md5(file_content).hexdigest()[:8]


# æ–‡ä»¶è§£æå‡½æ•°
def parse_file(file):
    content = ""
    file_type = file.name.split(".")[-1].lower()

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        if file_type == "txt":
            with open(tmp_path, "r", encoding="utf-8") as f:
                content = f.read()

        elif file_type == "pdf":
            reader = PdfReader(tmp_path)
            for page in reader.pages:
                content += page.extract_text() + "\n"

        elif file_type == "docx":
            doc = Document(tmp_path)
            for para in doc.paragraphs:
                content += para.text + "\n"

        elif file_type == "pptx":
            prs = pptx.Presentation(tmp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"

        os.unlink(tmp_path)
        return content

    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None


# å°†æ–‡æ¡£å†…å®¹åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„ç‰‡æ®µ
def split_into_chunks(text, chunk_size=200):
    chunks = []
    words = text.split()

    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i + chunk_size]))

    return chunks


# åˆ é™¤æ–‡ä»¶å¤„ç†
def delete_file(file_id):
    # ä»å·²ä¸Šä¼ æ–‡ä»¶ä¸­åˆ é™¤
    st.session_state.uploaded_files = [f for f in st.session_state.uploaded_files if f['id'] != file_id]

    # æ·»åŠ åˆ°åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºæ¢å¤ï¼‰
    deleted_file = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if not deleted_file:
        file_to_delete = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)
        if file_to_delete:
            st.session_state.deleted_files.append({
                **file_to_delete,
                'deleted_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })

    # ä»çŸ¥è¯†åº“ä¸­åˆ é™¤ç›¸å…³ç‰‡æ®µ
    st.session_state.knowledge_base = [kb for kb in st.session_state.knowledge_base if kb['source_id'] != file_id]


# æ¢å¤å·²åˆ é™¤æ–‡ä»¶
def restore_file(file_id):
    # ä»åˆ é™¤åˆ—è¡¨ä¸­æ¢å¤
    file_to_restore = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if file_to_restore:
        st.session_state.uploaded_files.append({
            'id': file_to_restore['id'],
            'name': file_to_restore['name'],
            'type': file_to_restore['type'],
            'content': file_to_restore['content'],
            'upload_time': file_to_restore['upload_time'],
            'tags': file_to_restore['tags'] if 'tags' in file_to_restore else []
        })

        # ä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
        st.session_state.deleted_files = [f for f in st.session_state.deleted_files if f['id'] != file_id]

        # é‡æ–°æ·»åŠ åˆ°çŸ¥è¯†åº“
        chunks = split_into_chunks(file_to_restore['content'])
        for i, chunk in enumerate(chunks):
            st.session_state.knowledge_base.append({
                "source": file_to_restore['name'],
                "source_id": file_to_restore['id'],
                "content": chunk,
                "type": file_to_restore['type'].split("/")[-1]
            })
        return True
    return False


# æ ‡è®°æ–‡ä»¶åŠŸèƒ½
def toggle_file_tag(file_id, tag):
    for file in st.session_state.uploaded_files:
        if file['id'] == file_id:
            if 'tags' not in file:
                file['tags'] = []

            if tag in file['tags']:
                file['tags'].remove(tag)
            else:
                file['tags'].append(tag)
            return True
    return False


# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢ï¼ˆæ·»åŠ æ–‡ä»¶åˆ é™¤å’Œæ ‡è®°åŠŸèƒ½ï¼‰
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è¿‡ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            file_id = generate_file_id(file.getvalue())
            existing_file = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)

            if not existing_file:
                with st.spinner(f"è§£ææ–‡ä»¶: {file.name}..."):
                    content = parse_file(file)

                    if content:
                        st.session_state.uploaded_files.append({
                            "id": file_id,
                            "name": file.name,
                            "type": file.type,
                            "content": content,
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "tags": ["æ–°ä¸Šä¼ "]  # é»˜è®¤æ ‡è®°
                        })

                        # åˆ†å‰²å†…å®¹ä¸ºçŸ¥è¯†ç‰‡æ®µ
                        chunks = split_into_chunks(content)
                        for i, chunk in enumerate(chunks):
                            st.session_state.knowledge_base.append({
                                "source": file.name,
                                "source_id": file_id,
                                "content": chunk,
                                "type": file.type.split("/")[-1]
                            })
        st.rerun()

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
    st.subheader("æ–‡æ¡£ç®¡ç†")
    st.info("ä½¿ç”¨ä»¥ä¸‹è¡¨æ ¼ç®¡ç†æ‚¨çš„æ–‡æ¡£ï¼š")

    files_df = pd.DataFrame([
        {
            "ID": f["id"],
            "æ–‡ä»¶å": f["name"],
            "ç±»å‹": f["type"],
            "å¤§å°": f"{len(f['content']) // 1024} KB",
            "ä¸Šä¼ æ—¶é—´": f["upload_time"],
            "æ ‡è®°": ", ".join(f.get("tags", [])) if "tags" in f else "",
        }
        for f in st.session_state.uploaded_files
    ])

    # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡ä»¶æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if files_df.empty:
        st.info("æš‚æ— æ–‡æ¡£ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")
    else:
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(files_df.set_index('ID'), use_container_width=True)

        # æ–‡ä»¶ç®¡ç†åŠŸèƒ½åŒº
        with st.expander("ğŸ“Œ æ–‡ä»¶æ“ä½œ", expanded=True):
            col1, col2, col3, col4 = st.columns([0.4, 0.2, 0.2, 0.2])

            # æ–‡ä»¶é€‰æ‹©
            file_ids = list(files_df["ID"])
            selected_file_id = col1.selectbox("é€‰æ‹©æ–‡ä»¶", file_ids, format_func=lambda id:
            files_df.loc[files_df["ID"] == id, "æ–‡ä»¶å"].values[0])

            # æ ‡è®°ç®¡ç†
            tags = ["é‡è¦", "å¾…å®¡æ ¸", "å­˜æ¡£", "å‚è€ƒ"]
            selected_tag = col2.selectbox("æ·»åŠ /ç§»é™¤æ ‡è®°", tags)

            # æŒ‰é’®æ“ä½œ
            if col3.button("åº”ç”¨æ ‡è®°", key="tag_btn", use_container_width=True):
                toggle_file_tag(selected_file_id, selected_tag)
                st.rerun()

            if col4.button("åˆ é™¤æ–‡ä»¶", key="delete_btn", type="primary", use_container_width=True):
                delete_file(selected_file_id)
                st.rerun()

        # å›æ”¶ç«™ç®¡ç†
        if st.session_state.deleted_files:
            with st.expander("ğŸ—‘ï¸ å›æ”¶ç«™ç®¡ç†", expanded=True):
                deleted_df = pd.DataFrame([
                    {
                        "ID": f["id"],
                        "æ–‡ä»¶å": f["name"],
                        "ç±»å‹": f["type"],
                        "åˆ é™¤æ—¶é—´": f["deleted_time"],
                    }
                    for f in st.session_state.deleted_files
                ])

                st.dataframe(deleted_df.set_index('ID'), use_container_width=True)

                restore_id = st.selectbox("é€‰æ‹©è¦æ¢å¤çš„æ–‡ä»¶", deleted_df["ID"], format_func=lambda id:
                deleted_df.loc[deleted_df["ID"] == id, "æ–‡ä»¶å"].values[0])

                if st.button("æ¢å¤æ–‡ä»¶", use_container_width=True):
                    if restore_file(restore_id):
                        st.success(f"æ–‡ä»¶å·²æ¢å¤")
                        st.rerun()
                    else:
                        st.error("æ¢å¤æ–‡ä»¶å¤±è´¥")

                if st.button("æ¸…ç©ºå›æ”¶ç«™", type="primary", use_container_width=True):
                    st.session_state.deleted_files = []
                    st.success("å›æ”¶ç«™å·²æ¸…ç©º")
                    st.rerun()

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("ğŸ” æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹", expanded=False):
            if not st.session_state.knowledge_base:
                st.info("çŸ¥è¯†åº“ä¸ºç©º")
            else:
                # åˆ†ç»„æ˜¾ç¤ºæŒ‰æ–‡ä»¶
                source_files = set(kb['source_id'] for kb in st.session_state.knowledge_base)
                for src_id in list(source_files)[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„å†…å®¹
                    source_name = next(
                        kb['source'] for kb in st.session_state.knowledge_base if kb['source_id'] == src_id)
                    st.subheader(f"æ¥æº: {source_name}")

                    # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªç‰‡æ®µ
                    for kb in [k for k in st.session_state.knowledge_base if k['source_id'] == src_id][:3]:
                        with st.expander(f"çŸ¥è¯†ç‰‡æ®µ {kb['content'][:30]}...", expanded=False):
                            st.markdown(kb["content"])
                    st.divider()
                if len(source_files) > 3:
                    st.info(f"å·²æ˜¾ç¤º3ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå…±{len(source_files)}ä¸ªæ–‡ä»¶")


# é—®ç­”ç•Œé¢ï¼ˆä¿æŒä¸å˜ï¼‰
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    # æ˜¾ç¤ºå†å²å¯¹è¯
    if st.session_state.conversation:
        st.subheader("å¯¹è¯å†å²")
        for i, message in enumerate(st.session_state.conversation):
            role = "ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·" if i % 2 == 0 else "ğŸ¤– ç³»ç»Ÿ"
            with st.chat_message(name=role):
                st.write(message)
            if i < len(st.session_state.conversation) - 1:
                st.divider()

    # ç”¨æˆ·è¾“å…¥é—®é¢˜
    question = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")

    if question:
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        with st.chat_message(name="ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·"):
            st.write(question)

        # æ¨¡æ‹Ÿæ£€ç´¢å’Œç”Ÿæˆç­”æ¡ˆçš„è¿‡ç¨‹
        with st.spinner("æ­£åœ¨æ€è€ƒå¹¶ç”Ÿæˆç­”æ¡ˆ..."):
            time.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

            # ç®€åŒ–çš„æ£€ç´¢è¿‡ç¨‹
            matching_chunks = []
            for item in st.session_state.knowledge_base:
                if any(word in item['content'] for word in question.split()[:3]):
                    matching_chunks.append(item)

            # æ¨¡æ‹Ÿç”Ÿæˆç­”æ¡ˆ
            if matching_chunks:
                answer = f"æ ¹æ® **{matching_chunks[0]['source']}** ä¸­çš„å†…å®¹ä¸ºæ‚¨è§£ç­”ï¼š\n\n"
                answer += matching_chunks[0]['content'][:300] + "\n\n"
                answer += "*(æ­¤ä¸ºæ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…ç³»ç»Ÿä¼šç»“åˆä¸Šä¸‹æ–‡ç”Ÿæˆæ›´è‡ªç„¶çš„å›ç­”)*"
            else:
                answer = "åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜æˆ–ä¸Šä¼ æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚"

            # æ·»åŠ åˆ°å¯¹è¯å†å²
            st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºç­”æ¡ˆ
            with st.chat_message(name="ğŸ¤– ç³»ç»Ÿ"):
                st.write(answer)


# ä¸»ç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ“š åŸºäºRAGçš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.caption("çŸ¥è¯†åº“æ„å»ºã€ç®¡ç†åŠæ™ºèƒ½é—®ç­”å¹³å° | æ”¯æŒæ–‡æ¡£å¤„ç†ä¸åˆ†æ")

    init_session()

    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header("ğŸ” å¯¼èˆªèœå•")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"], horizontal=True)
        st.divider()

        # ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ
        st.subheader("ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        col1.metric("çŸ¥è¯†æ–‡æ¡£", len(st.session_state.uploaded_files))
        col2.metric("çŸ¥è¯†ç‰‡æ®µ", len(st.session_state.knowledge_base))
        col3.metric("å›æ”¶ç«™", len(st.session_state.deleted_files))
        st.divider()

        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        1. çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†
        2. æ–‡æ¡£è§£æä¸å¤„ç†
        3. è‡ªç„¶è¯­è¨€é—®ç­”
        4. æ–‡ä»¶æ ‡è®°ä¸å›æ”¶
        """)

        # æ•°æ®ç®¡ç†é€‰é¡¹
        if st.button("æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True, type="secondary"):
            st.session_state.uploaded_files = []
            st.session_state.knowledge_base = []
            st.session_state.conversation = []
            st.rerun()

    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()

    # è°ƒè¯•ä¿¡æ¯
    with st.expander("ğŸ› ï¸ è°ƒè¯•ä¿¡æ¯", expanded=False):
        st.json({
            "æ–‡ä»¶ä¸Šä¼ æ•°": len(st.session_state.uploaded_files),
            "çŸ¥è¯†ç‰‡æ®µæ•°": len(st.session_state.knowledge_base),
            "åˆ é™¤æ–‡ä»¶æ•°": len(st.session_state.deleted_files),
            "å¯¹è¯è½®æ¬¡": len(st.session_state.conversation) // 2
        })


if __name__ == "__main__":
    main()