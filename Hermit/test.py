import streamlit as st
import time
from datetime import datetime
import os
import pandas as pd
import json
from docx import Document
from PyPDF2 import PdfReader
import pptx
import tempfile
import hashlib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer

# ä¸‹è½½NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œæ—¶éœ€è¦ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')


# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = None
    if "knowledge_vectors" not in st.session_state:
        st.session_state.knowledge_vectors = []


# åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')


# ç”Ÿæˆæ–‡ä»¶å”¯ä¸€ID
def generate_file_id(file_content):
    return hashlib.md5(file_content).hexdigest()[:8]


# æ–‡ä»¶è§£æå‡½æ•°
def parse_file(file):
    content = ""
    file_type = file.name.split(".")[-1].lower()

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        if file_type == "txt":
            with open(tmp_path, "r", encoding="utf-8") as f:
                content = f.read()

        elif file_type == "pdf":
            reader = PdfReader(tmp_path)
            for page in reader.pages:
                content += page.extract_text() + "\n"

        elif file_type == "docx":
            doc = Document(tmp_path)
            for para in doc.paragraphs:
                content += para.text + "\n"

        elif file_type == "pptx":
            prs = pptx.Presentation(tmp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"

        os.unlink(tmp_path)
        return content

    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None


# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    text = re.sub(r'[^a-zA-Z\u4e00-\u9fff\s]', '', text)
    # åˆ†è¯
    tokens = word_tokenize(text)
    # ç§»é™¤åœç”¨è¯
    stop_words = set(stopwords.words('english') + stopwords.words('chinese'))
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)


# å°†æ–‡æ¡£å†…å®¹åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„ç‰‡æ®µ
def split_into_chunks(text, chunk_size=200):
    chunks = []
    words = text.split()

    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i + chunk_size]))

    return chunks


# è¯­ä¹‰åˆ†æå‡½æ•°
def semantic_analysis(question):
    # 1. æ„å›¾è¯†åˆ«
    intent = "ä¿¡æ¯æŸ¥è¯¢"  # é»˜è®¤æ„å›¾

    if any(word in question.lower() for word in ["å¦‚ä½•", "æ€æ ·", "æ­¥éª¤"]):
        intent = "æ“ä½œæŒ‡å¯¼"
    elif any(word in question.lower() for word in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "ä¸ºä½•"]):
        intent = "åŸå› è§£é‡Š"
    elif any(word in question.lower() for word in ["æ¯”è¾ƒ", "å¯¹æ¯”", "vs"]):
        intent = "æ¯”è¾ƒåˆ†æ"
    elif any(word in question.lower() for word in ["æ¨è", "å»ºè®®", "åº”è¯¥"]):
        intent = "æ¨èå»ºè®®"

    # 2. å…³é”®å®ä½“æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„å®ä½“
    entities = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{3,}', question)

    # 3. ç”Ÿæˆè¯­ä¹‰å‘é‡
    model = st.session_state.embedding_model
    embedding = model.encode([question])[0]

    return {
        "intent": intent,
        "entities": entities,
        "embedding": embedding
    }


# åˆ é™¤æ–‡ä»¶å¤„ç†
def delete_file(file_id):
    # ä»å·²ä¸Šä¼ æ–‡ä»¶ä¸­åˆ é™¤
    st.session_state.uploaded_files = [f for f in st.session_state.uploaded_files if f['id'] != file_id]

    # æ·»åŠ åˆ°åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºæ¢å¤ï¼‰
    deleted_file = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if not deleted_file:
        file_to_delete = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)
        if file_to_delete:
            st.session_state.deleted_files.append({
                **file_to_delete,
                'deleted_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })

    # ä»çŸ¥è¯†åº“ä¸­åˆ é™¤ç›¸å…³ç‰‡æ®µ
    st.session_state.knowledge_base = [kb for kb in st.session_state.knowledge_base if kb['source_id'] != file_id]

    # æ›´æ–°å‘é‡å­˜å‚¨
    update_vector_store()


# æ¢å¤å·²åˆ é™¤æ–‡ä»¶
def restore_file(file_id):
    # ä»åˆ é™¤åˆ—è¡¨ä¸­æ¢å¤
    file_to_restore = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if file_to_restore:
        st.session_state.uploaded_files.append({
            'id': file_to_restore['id'],
            'name': file_to_restore['name'],
            'type': file_to_restore['type'],
            'content': file_to_restore['content'],
            'upload_time': file_to_restore['upload_time'],
            'tags': file_to_restore['tags'] if 'tags' in file_to_restore else []
        })

        # ä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
        st.session_state.deleted_files = [f for f in st.session_state.deleted_files if f['id'] != file_id]

        # é‡æ–°æ·»åŠ åˆ°çŸ¥è¯†åº“
        chunks = split_into_chunks(file_to_restore['content'])
        for i, chunk in enumerate(chunks):
            st.session_state.knowledge_base.append({
                "source": file_to_restore['name'],
                "source_id": file_to_restore['id'],
                "content": chunk,
                "type": file_to_restore['type'].split("/")[-1]
            })

        # æ›´æ–°å‘é‡å­˜å‚¨
        update_vector_store()
        return True
    return False


# æ ‡è®°æ–‡ä»¶åŠŸèƒ½
def toggle_file_tag(file_id, tag):
    for file in st.session_state.uploaded_files:
        if file['id'] == file_id:
            if 'tags' not in file:
                file['tags'] = []

            if tag in file['tags']:
                file['tags'].remove(tag)
            else:
                file['tags'].append(tag)
            return True
    return False


# æ›´æ–°å‘é‡å­˜å‚¨
def update_vector_store():
    if st.session_state.embedding_model and st.session_state.knowledge_base:
        model = st.session_state.embedding_model
        contents = [kb["content"] for kb in st.session_state.knowledge_base]
        st.session_state.knowledge_vectors = model.encode(contents)


# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢ï¼ˆæ·»åŠ æ–‡ä»¶åˆ é™¤å’Œæ ‡è®°åŠŸèƒ½ï¼‰
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # ç¡®ä¿åµŒå…¥æ¨¡å‹å·²åŠ è½½
        if not st.session_state.embedding_model:
            with st.spinner("åŠ è½½è¯­ä¹‰æ¨¡å‹..."):
                st.session_state.embedding_model = load_embedding_model()

        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è¿‡ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            file_id = generate_file_id(file.getvalue())
            existing_file = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)

            if not existing_file:
                with st.spinner(f"è§£ææ–‡ä»¶: {file.name}..."):
                    content = parse_file(file)

                    if content:
                        st.session_state.uploaded_files.append({
                            "id": file_id,
                            "name": file.name,
                            "type": file.type,
                            "content": content,
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "tags": ["æ–°ä¸Šä¼ "]  # é»˜è®¤æ ‡è®°
                        })

                        # åˆ†å‰²å†…å®¹ä¸ºçŸ¥è¯†ç‰‡æ®µ
                        chunks = split_into_chunks(content)
                        for i, chunk in enumerate(chunks):
                            st.session_state.knowledge_base.append({
                                "source": file.name,
                                "source_id": file_id,
                                "content": chunk,
                                "type": file.type.split("/")[-1]
                            })

        # æ›´æ–°å‘é‡å­˜å‚¨
        update_vector_store()
        st.rerun()

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
    st.subheader("æ–‡æ¡£ç®¡ç†")
    st.info("ä½¿ç”¨ä»¥ä¸‹è¡¨æ ¼ç®¡ç†æ‚¨çš„æ–‡æ¡£ï¼š")

    files_df = pd.DataFrame([
        {
            "ID": f["id"],
            "æ–‡ä»¶å": f["name"],
            "ç±»å‹": f["type"],
            "å¤§å°": f"{len(f['content']) // 1024} KB",
            "ä¸Šä¼ æ—¶é—´": f["upload_time"],
            "æ ‡è®°": ", ".join(f.get("tags", [])) if "tags" in f else "",
        }
        for f in st.session_state.uploaded_files
    ])

    # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡ä»¶æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if files_df.empty:
        st.info("æš‚æ— æ–‡æ¡£ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")
    else:
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(files_df.set_index('ID'), use_container_width=True)

        # æ–‡ä»¶ç®¡ç†åŠŸèƒ½åŒº
        with st.expander("ğŸ“Œ æ–‡ä»¶æ“ä½œ", expanded=True):
            col1, col2, col3, col4 = st.columns([0.4, 0.2, 0.2, 0.2])

            # æ–‡ä»¶é€‰æ‹©
            file_ids = list(files_df["ID"])
            selected_file_id = col1.selectbox("é€‰æ‹©æ–‡ä»¶", file_ids, format_func=lambda id:
            files_df.loc[files_df["ID"] == id, "æ–‡ä»¶å"].values[0])

            # æ ‡è®°ç®¡ç†
            tags = ["é‡è¦", "å¾…å®¡æ ¸", "å­˜æ¡£", "å‚è€ƒ"]
            selected_tag = col2.selectbox("æ·»åŠ /ç§»é™¤æ ‡è®°", tags)

            # æŒ‰é’®æ“ä½œ
            if col3.button("åº”ç”¨æ ‡è®°", key="tag_btn", use_container_width=True):
                toggle_file_tag(selected_file_id, selected_tag)
                st.rerun()

            if col4.button("åˆ é™¤æ–‡ä»¶", key="delete_btn", type="primary", use_container_width=True):
                delete_file(selected_file_id)
                st.rerun()

        # å›æ”¶ç«™ç®¡ç†
        if st.session_state.deleted_files:
            with st.expander("ğŸ—‘ï¸ å›æ”¶ç«™ç®¡ç†", expanded=True):
                deleted_df = pd.DataFrame([
                    {
                        "ID": f["id"],
                        "æ–‡ä»¶å": f["name"],
                        "ç±»å‹": f["type"],
                        "åˆ é™¤æ—¶é—´": f["deleted_time"],
                    }
                    for f in st.session_state.deleted_files
                ])

                st.dataframe(deleted_df.set_index('ID'), use_container_width=True)

                restore_id = st.selectbox("é€‰æ‹©è¦æ¢å¤çš„æ–‡ä»¶", deleted_df["ID"], format_func=lambda id:
                deleted_df.loc[deleted_df["ID"] == id, "æ–‡ä»¶å"].values[0])

                if st.button("æ¢å¤æ–‡ä»¶", use_container_width=True):
                    if restore_file(restore_id):
                        st.success(f"æ–‡ä»¶å·²æ¢å¤")
                        st.rerun()
                    else:
                        st.error("æ¢å¤æ–‡ä»¶å¤±è´¥")

                if st.button("æ¸…ç©ºå›æ”¶ç«™", type="primary", use_container_width=True):
                    st.session_state.deleted_files = []
                    st.success("å›æ”¶ç«™å·²æ¸…ç©º")
                    st.rerun()

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("ğŸ” æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹", expanded=False):
            if not st.session_state.knowledge_base:
                st.info("çŸ¥è¯†åº“ä¸ºç©º")
            else:
                # åˆ†ç»„æ˜¾ç¤ºæŒ‰æ–‡ä»¶
                source_files = set(kb['source_id'] for kb in st.session_state.knowledge_base)
                for src_id in list(source_files)[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„å†…å®¹
                    source_name = next(
                        kb['source'] for kb in st.session_state.knowledge_base if kb['source_id'] == src_id)
                    st.subheader(f"æ¥æº: {source_name}")

                    # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªç‰‡æ®µ
                    for kb in [k for k in st.session_state.knowledge_base if k['source_id'] == src_id][:3]:
                        with st.expander(f"çŸ¥è¯†ç‰‡æ®µ {kb['content'][:30]}...", expanded=False):
                            st.markdown(kb["content"])
                    st.divider()
                if len(source_files) > 3:
                    st.info(f"å·²æ˜¾ç¤º3ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå…±{len(source_files)}ä¸ªæ–‡ä»¶")


# é—®ç­”ç•Œé¢ï¼ˆæ·»åŠ è¯­ä¹‰ç†è§£åŠŸèƒ½ï¼‰
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (è¯­ä¹‰ç†è§£ç‰ˆ)")

    # æ˜¾ç¤ºå†å²å¯¹è¯
    if st.session_state.conversation:
        st.subheader("å¯¹è¯å†å²")
        for i, message in enumerate(st.session_state.conversation):
            role = "ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·" if i % 2 == 0 else "ğŸ¤– ç³»ç»Ÿ"
            with st.chat_message(name=role):
                st.write(message)
            if i < len(st.session_state.conversation) - 1:
                st.divider()

    # ç”¨æˆ·è¾“å…¥é—®é¢˜
    question = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")

    if question:
        # ç¡®ä¿åµŒå…¥æ¨¡å‹å·²åŠ è½½
        if not st.session_state.embedding_model:
            with st.spinner("åŠ è½½è¯­ä¹‰æ¨¡å‹..."):
                st.session_state.embedding_model = load_embedding_model()

        # æ·»åŠ åˆ°å¯¹è¯å†å²
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        with st.chat_message(name="ğŸ•µï¸â€â™‚ï¸ ç”¨æˆ·"):
            st.write(question)

        # è¯­ä¹‰åˆ†æå¤„ç†
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜è¯­ä¹‰..."):
            semantic_info = semantic_analysis(question)
            intent = semantic_info["intent"]
            entities = semantic_info["entities"]
            question_embedding = semantic_info["embedding"]

        # è¯­ä¹‰æ£€ç´¢è¿‡ç¨‹
        with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³çŸ¥è¯†..."):
            # ä¸ºçŸ¥è¯†åº“ç‰‡æ®µç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå¦‚æœå°šæœªç”Ÿæˆï¼‰
            if st.session_state.knowledge_vectors.size == 0 and st.session_state.knowledge_base:
                model = st.session_state.embedding_model
                contents = [kb["content"] for kb in st.session_state.knowledge_base]
                st.session_state.knowledge_vectors = model.encode(contents)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for idx, kb_vector in enumerate(st.session_state.knowledge_vectors):
                similarity = cosine_similarity([question_embedding], [kb_vector])[0][0]
                similarities.append((idx, similarity))

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)

            # è·å–æœ€ç›¸å…³çš„å‰3ä¸ªç‰‡æ®µ
            top_matches = []
            for idx, score in similarities[:3]:
                if score > 0.3:  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
                    top_matches.append(st.session_state.knowledge_base[idx])

        # ç”Ÿæˆç­”æ¡ˆ
        with st.spinner("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."):
            if top_matches:
                answer = f"### é—®é¢˜åˆ†æ\n"
                answer += f"- **æ„å›¾è¯†åˆ«**: {intent}\n"
                if entities:
                    answer += f"- **å…³é”®å®ä½“**: {', '.join(entities)}\n"

                answer += f"\n### æ ¹æ®ç›¸å…³çŸ¥è¯†ä¸ºæ‚¨è§£ç­”\n"
                for i, match in enumerate(top_matches):
                    answer += f"**æ¥æº {i + 1}** ({match['source']}):\n"
                    answer += f"> {match['content'][:200]}...\n\n"

                # æ ¹æ®ä¸åŒæ„å›¾æ·»åŠ é¢å¤–ä¿¡æ¯
                if intent == "æ¯”è¾ƒåˆ†æ":
                    answer += "\n> *æç¤ºï¼šå¦‚éœ€è¯¦ç»†æ¯”è¾ƒåˆ†æï¼Œè¯·æä¾›æ›´å¤šå…·ä½“ä¿¡æ¯*"
                elif intent == "æ¨èå»ºè®®":
                    answer += "\n> *æç¤ºï¼šä»¥ä¸Šä¸ºåŸºäºçŸ¥è¯†åº“çš„æ¨èï¼Œä»…ä¾›å‚è€ƒ*"
                elif intent == "æ“ä½œæŒ‡å¯¼":
                    answer += "\n> *æç¤ºï¼šå¦‚éœ€æ›´è¯¦ç»†çš„æ“ä½œæ­¥éª¤æŒ‡å—ï¼Œè¯·è¡¥å……å…·ä½“åœºæ™¯ä¿¡æ¯*"
            else:
                answer = "åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜æˆ–ä¸Šä¼ æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚"

            # æ·»åŠ åˆ°å¯¹è¯å†å²
            st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºç­”æ¡ˆ
            with st.chat_message(name="ğŸ¤– ç³»ç»Ÿ"):
                st.markdown(answer)

            # è°ƒè¯•ä¿¡æ¯
            with st.expander("ğŸ” è¯­ä¹‰åˆ†æè¯¦æƒ…", expanded=False):
                st.json({
                    "é—®é¢˜æ„å›¾": intent,
                    "è¯†åˆ«å®ä½“": entities,
                    "åŒ¹é…ç‰‡æ®µæ•°": len(top_matches),
                    "æœ€é«˜ç›¸ä¼¼åº¦": similarities[0][1] if similarities else 0
                })


# ä¸»ç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸ“š åŸºäºè¯­ä¹‰ç†è§£çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    st.caption("çŸ¥è¯†åº“æ„å»ºã€ç®¡ç†åŠæ™ºèƒ½é—®ç­”å¹³å° | æ”¯æŒæ–‡æ¡£å¤„ç†ä¸è¯­ä¹‰åˆ†æ")

    init_session()

    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header("ğŸ” å¯¼èˆªèœå•")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"], horizontal=True)
        st.divider()

        # ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ
        st.subheader("ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        col1.metric("çŸ¥è¯†æ–‡æ¡£", len(st.session_state.uploaded_files))
        col2.metric("çŸ¥è¯†ç‰‡æ®µ", len(st.session_state.knowledge_base))
        col3.metric("å›æ”¶ç«™", len(st.session_state.deleted_files))
        st.divider()

        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        1. çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†
        2. æ–‡æ¡£è§£æä¸å¤„ç†
        3. è¯­ä¹‰åˆ†æä¸ç†è§£
        4. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        5. æ–‡ä»¶æ ‡è®°ä¸å›æ”¶
        """)

        # æ•°æ®ç®¡ç†é€‰é¡¹
        if st.button("æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True, type="secondary"):
            st.session_state.uploaded_files = []
            st.session_state.knowledge_base = []
            st.session_state.conversation = []
            st.session_state.knowledge_vectors = []
            st.rerun()

    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()

    # è°ƒè¯•ä¿¡æ¯
    with st.expander("ğŸ› ï¸ è°ƒè¯•ä¿¡æ¯", expanded=False):
        st.json({
            "æ–‡ä»¶ä¸Šä¼ æ•°": len(st.session_state.uploaded_files),
            "çŸ¥è¯†ç‰‡æ®µæ•°": len(st.session_state.knowledge_base),
            "å‘é‡å­˜å‚¨æ•°": len(st.session_state.knowledge_vectors),
            "åˆ é™¤æ–‡ä»¶æ•°": len(st.session_state.deleted_files),
            "å¯¹è¯è½®æ¬¡": len(st.session_state.conversation) // 2
        })


if __name__ == "__main__":
    main()