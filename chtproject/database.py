import streamlit as st
import time
from datetime import datetime
import os
import pandas as pd
from docx import Document
from PyPDF2 import PdfReader
import pptx
import tempfile
import hashlib
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai


# ==================== æ–°å¢å¼€å§‹ ====================
# DeepSeek AI æ¥å£é…ç½®
# ç”Ÿæˆæ–‡ä»¶å”¯ä¸€IDï¼ˆæ·»åŠ åœ¨ init_session() å‡½æ•°ä¹‹åï¼‰
def generate_file_id(file_content):
    """é€šè¿‡æ–‡ä»¶å†…å®¹ç”Ÿæˆå”¯ä¸€å“ˆå¸ŒID"""
    return hashlib.md5(file_content).hexdigest()[:8]

# æ–‡ä»¶è§£æå‡½æ•°ï¼ˆæ·»åŠ åœ¨ generate_file_id() ä¹‹åï¼‰
def parse_file(file):
    """è§£æä¸Šä¼ çš„æ–‡ä»¶å†…å®¹"""
    content = ""
    file_type = file.name.split(".")[-1].lower()

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        if file_type == "txt":
            with open(tmp_path, "r", encoding="utf-8") as f:
                content = f.read()

        elif file_type == "pdf":
            reader = PdfReader(tmp_path)
            for page in reader.pages:
                content += page.extract_text() + "\n"

        elif file_type == "docx":
            doc = Document(tmp_path)
            for para in doc.paragraphs:
                content += para.text + "\n"

        elif file_type == "pptx":
            prs = pptx.Presentation(tmp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"

        os.unlink(tmp_path)
        return content

    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None
def setup_deepseek():
    openai.api_key = "sk-g40Ua40lLiQhMcEN1b710a5d63E14bD89921Ed47D8B371Fb"  # ä»secretsè·å–ï¼Œä¸å­˜åœ¨åˆ™ä¸ºç©º
    openai.base_url = "https://api.gpt.ge/v1/"


def ask_ai(question, model="deepseek-chat"):
    """è°ƒç”¨DeepSeek AIæ¥å£"""
    try:
        completion = openai.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©è¿›è¡Œæ–‡çŒ®ç®¡ç†çš„AIé—®ç­”ç³»ç»Ÿã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç”¨ç²¾ç‚¼è€Œç§‘å­¦çš„è¯­è¨€å›ç­”é—®é¢˜"
                },
                {"role": "user", "content": question},
            ],
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"AIæ¥å£è°ƒç”¨å¤±è´¥: {str(e)}")
        return "æ— æ³•è·å–AIå›ç­”ï¼Œè¯·æ£€æŸ¥APIé…ç½®"


# ==================== æ–°å¢ç»“æŸ ====================

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []
    if "vector_db" not in st.session_state:
        st.session_state.embedding_model = HuggingFaceEmbeddings(
            model_name="GanymedeNil/text2vec-large-chinese",
            model_kwargs={'device': 'cpu'}
        )
        st.session_state.vector_db = Chroma(
            embedding_function=st.session_state.embedding_model,
            persist_directory="./chroma_db"
        )
    if "text_splitter" not in st.session_state:
        st.session_state.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )


# [ä¿ç•™æ‚¨åŸæœ‰çš„ parse_fileã€generate_file_idã€delete_file ç­‰è¾…åŠ©å‡½æ•°...]

# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒPDF/DOCX/PPTX/TXT)",
        type=["pdf", "docx", "pptx", "txt"],
        accept_multiple_files=True
    )

    if uploaded_files:
        for file in uploaded_files:
            file_id = generate_file_id(file.getvalue())
            if not next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None):
                content = parse_file(file)
                if content:
                    chunks = st.session_state.text_splitter.split_text(content)

                    # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                    st.session_state.vector_db.add_texts(
                        texts=chunks,
                        metadatas=[{
                            "source": file.name,
                            "source_id": file_id,
                            "type": file.type.split("/")[-1],
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        } for _ in chunks]
                    )

                    # ä¿ç•™åˆ°çŸ¥è¯†åº“
                    st.session_state.uploaded_files.append({
                        "id": file_id,
                        "name": file.name,
                        "type": file.type,
                        "content": content,
                        "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "tags": ["æ–°ä¸Šä¼ "]
                    })

    # [ä¿ç•™æ‚¨åŸæœ‰çš„æ–‡ä»¶åˆ—è¡¨å±•ç¤ºã€åˆ é™¤/æ¢å¤ç­‰UIé€»è¾‘...]


# ==================== ä¿®æ”¹å¼€å§‹ ====================
# å¢å¼ºç‰ˆé—®ç­”ç•Œé¢ï¼ˆé›†æˆDeepSeekï¼‰
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    # æ˜¾ç¤ºå¯¹è¯å†å²
    if st.session_state.conversation:
        for msg in st.session_state.conversation:
            role = "user" if msg.startswith("ç”¨æˆ·:") else "assistant"
            with st.chat_message(role):
                st.write(msg.split(":", 1)[1].strip())

    # ç”¨æˆ·æé—®å¤„ç†
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        # 1. å‘é‡æ£€ç´¢
        docs = st.session_state.vector_db.similarity_search(question, k=3)

        # 2. æ„å»ºç§‘å­¦é—®ç­”æç¤ºè¯
        context = "\n".join([
            f"ã€æ–‡çŒ® {i + 1}ã€‘{doc.metadata['source']}\n{doc.page_content}\n"
            for i, doc in enumerate(docs)
        ])

        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ï¼š
{context}
é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
1. å›ç­”éœ€å¼•ç”¨æ–‡çŒ®ï¼ˆä¾‹ï¼šã€æ–‡çŒ®1ã€‘ï¼‰
2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§
3. å¦‚æ— ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜
å›ç­”ï¼š"""

        # 3. è°ƒç”¨DeepSeekç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                answer = ask_ai(prompt)
                st.write(answer)
                st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
            with st.expander("ğŸ“š å‚è€ƒæ–‡æ¡£", expanded=False):
                for i, doc in enumerate(docs, 1):
                    st.caption(f"ã€æ–‡çŒ®{i}ã€‘{doc.metadata['source']}")
                    st.text(doc.page_content[:200] + "...")


# ==================== ä¿®æ”¹ç»“æŸ ====================

# ä¸»ç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ“š",
        layout="wide"
    )

    setup_deepseek()  # åˆå§‹åŒ–DeepSeeké…ç½®
    init_session()

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("å¯¼èˆªèœå•")
        page = st.radio("åŠŸèƒ½é€‰æ‹©", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"])

        st.divider()
        st.info("""
        **ä½¿ç”¨è¯´æ˜ï¼š**
        1. ä¸Šä¼ æ–‡çŒ®åˆ°çŸ¥è¯†åº“
        2. åœ¨é—®ç­”é¡µæé—®
        3. ç³»ç»Ÿå°†ç»“åˆæ–‡çŒ®å›ç­”
        """)

    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()


if __name__ == "__main__":
    main()