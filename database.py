import streamlit as st
from Files_Operator import parse_file, generate_file_id, load_embedding_model
from Database_Operator import (
    delete_file,
    restore_file,
    toggle_file_tag,
    semantic_analysis,
    update_vector_store
)
from AI_Respond import ask_ai
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import pandas as pd



def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = None
    if "knowledge_vectors" not in st.session_state:
        st.session_state.knowledge_vectors = []
    if "vector_db" not in st.session_state:
        st.session_state.embedding_model_langchain = HuggingFaceEmbeddings(
            model_name="GanymedeNil/text2vec-large-chinese",
            model_kwargs={'device': 'cpu'}
        )
        st.session_state.vector_db = Chroma(
            embedding_function=st.session_state.embedding_model_langchain,
            persist_directory="./chroma_db"
        )
    if "text_splitter" not in st.session_state:
        st.session_state.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )


def knowledge_base_section():
    from UI import knowledge_base_section as kb_section
    kb_section()


def qa_interface():
    from UI import qa_interface as qa_func
    qa_func()


def main():
    st.set_page_config(
        page_title="æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    if "model_downloaded" not in st.session_state:
        with st.spinner("æ­£åœ¨é¢„ä¸‹è½½è¯­ä¹‰æ¨¡å‹ï¼ˆçº¦1.2GBï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦æ—¶é—´ï¼‰..."):
            try:
                model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                st.session_state.model_downloaded = True
                st.session_state.embedding_model = model
            except Exception as e:
                st.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
                return

    init_session()
    st.title("ğŸ“š æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ")
    st.caption("çŸ¥è¯†åº“æ„å»ºã€ç®¡ç†åŠæ™ºèƒ½é—®ç­”å¹³å° | æ”¯æŒæ–‡æ¡£å¤„ç†ä¸è¯­ä¹‰åˆ†æ")

    with st.sidebar:
        st.header("ğŸ” å¯¼èˆªèœå•")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"], horizontal=True)
        st.divider()
        st.subheader("ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        col1.metric("çŸ¥è¯†æ–‡æ¡£", len(st.session_state.uploaded_files))
        col2.metric("çŸ¥è¯†ç‰‡æ®µ", len(st.session_state.knowledge_base))
        col3.metric("å›æ”¶ç«™", len(st.session_state.deleted_files))
        st.divider()
        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        1. çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†
        2. æ–‡æ¡£è§£æä¸å¤„ç†
        3. è¯­ä¹‰åˆ†æä¸ç†è§£
        4. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        5. æ–‡ä»¶æ ‡è®°ä¸å›æ”¶
        """)
        if st.button("æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True, type="secondary"):
            st.session_state.uploaded_files = []
            st.session_state.knowledge_base = []
            st.session_state.conversation = []
            st.session_state.knowledge_vectors = []
            st.session_state.vector_db = Chroma(
                embedding_function=st.session_state.embedding_model_langchain,
                persist_directory="./chroma_db"
            )
            st.rerun()

    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()

    with st.expander("ğŸ› ï¸ è°ƒè¯•ä¿¡æ¯", expanded=False):
        st.json({
            "æ–‡ä»¶ä¸Šä¼ æ•°": len(st.session_state.uploaded_files),
            "çŸ¥è¯†ç‰‡æ®µæ•°": len(st.session_state.knowledge_base),
            "å‘é‡å­˜å‚¨æ•°": len(st.session_state.knowledge_vectors) if hasattr(st.session_state.knowledge_vectors, '__len__') else 0,
            "åˆ é™¤æ–‡ä»¶æ•°": len(st.session_state.deleted_files),
            "å¯¹è¯è½®æ¬¡": len(st.session_state.conversation) // 2
        })


if __name__ == "__main__":
    main()