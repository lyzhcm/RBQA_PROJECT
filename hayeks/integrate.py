import streamlit as st
import time
from datetime import datetime
import os
import pandas as pd
import json
from docx import Document
from PyPDF2 import PdfReader
import pptx
import tempfile
import hashlib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai

# ä¸‹è½½NLTKèµ„æºï¼ˆé¦–æ¬¡è¿è¡Œæ—¶éœ€è¦ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

# DeepSeek AI æ¥å£é…ç½®
def setup_deepseek():
    openai.api_key = "sk-g40Ua40lLiQhMcEN1b710a5d63E14bD89921Ed47D8B371Fb"  # ä»secretsè·å–ï¼Œä¸å­˜åœ¨åˆ™ä¸ºç©º
    openai.base_url = "https://api.gpt.ge/v1/"

def ask_ai(question, model="deepseek-chat"):
    """è°ƒç”¨DeepSeek AIæ¥å£"""
    try:
        completion = openai.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©è¿›è¡Œæ–‡çŒ®ç®¡ç†çš„AIé—®ç­”ç³»ç»Ÿã€‚æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç”¨ç²¾ç‚¼è€Œç§‘å­¦çš„è¯­è¨€å›ç­”é—®é¢˜"
                },
                {"role": "user", "content": question},
            ],
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"AIæ¥å£è°ƒç”¨å¤±è´¥: {str(e)}")
        return "æ— æ³•è·å–AIå›ç­”ï¼Œè¯·æ£€æŸ¥APIé…ç½®"

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = None
    if "knowledge_vectors" not in st.session_state:
        st.session_state.knowledge_vectors = []
    if "vector_db" not in st.session_state:
        st.session_state.embedding_model_langchain = HuggingFaceEmbeddings(
            model_name="GanymedeNil/text2vec-large-chinese",
            model_kwargs={'device': 'cpu'}
        )
        st.session_state.vector_db = Chroma(
            embedding_function=st.session_state.embedding_model_langchain,
            persist_directory="./chroma_db"
        )
    if "text_splitter" not in st.session_state:
        st.session_state.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )

# ç”Ÿæˆæ–‡ä»¶å”¯ä¸€ID
def generate_file_id(file_content):
    return hashlib.md5(file_content).hexdigest()[:8]

# æ–‡ä»¶è§£æå‡½æ•°
def parse_file(file):
    content = ""
    file_type = file.name.split(".")[-1].lower()

    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_type}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        if file_type == "txt":
            with open(tmp_path, "r", encoding="utf-8") as f:
                content = f.read()

        elif file_type == "pdf":
            reader = PdfReader(tmp_path)
            for page in reader.pages:
                content += page.extract_text() + "\n"

        elif file_type == "docx":
            doc = Document(tmp_path)
            for para in doc.paragraphs:
                content += para.text + "\n"

        elif file_type == "pptx":
            prs = pptx.Presentation(tmp_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"

        os.unlink(tmp_path)
        return content

    except Exception as e:
        st.error(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

# æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—
    text = re.sub(r'[^a-zA-Z\u4e00-\u9fff\s]', '', text)
    # åˆ†è¯
    tokens = word_tokenize(text)
    # ç§»é™¤åœç”¨è¯
    stop_words = set(stopwords.words('english') + stopwords.words('chinese'))
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

# å°†æ–‡æ¡£å†…å®¹åˆ†å‰²ä¸ºé€‚åˆå¤„ç†çš„ç‰‡æ®µ
def split_into_chunks(text, chunk_size=200):
    chunks = []
    words = text.split()

    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i + chunk_size]))

    return chunks

# è¯­ä¹‰åˆ†æå‡½æ•°
def semantic_analysis(question):
    # 1. æ„å›¾è¯†åˆ«
    intent = "ä¿¡æ¯æŸ¥è¯¢"  # é»˜è®¤æ„å›¾

    if any(word in question.lower() for word in ["å¦‚ä½•", "æ€æ ·", "æ­¥éª¤"]):
        intent = "æ“ä½œæŒ‡å¯¼"
    elif any(word in question.lower() for word in ["ä¸ºä»€ä¹ˆ", "åŸå› ", "ä¸ºä½•"]):
        intent = "åŸå› è§£é‡Š"
    elif any(word in question.lower() for word in ["æ¯”è¾ƒ", "å¯¹æ¯”", "vs"]):
        intent = "æ¯”è¾ƒåˆ†æ"
    elif any(word in question.lower() for word in ["æ¨è", "å»ºè®®", "åº”è¯¥"]):
        intent = "æ¨èå»ºè®®"

    # 2. å…³é”®å®ä½“æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„å®ä½“
    entities = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{3,}', question)

    # 3. ç”Ÿæˆè¯­ä¹‰å‘é‡
    model = st.session_state.embedding_model
    embedding = model.encode([question])[0]

    return {
        "intent": intent,
        "entities": entities,
        "embedding": embedding
    }

# åˆ é™¤æ–‡ä»¶å¤„ç†
def delete_file(file_id):
    # ä»å·²ä¸Šä¼ æ–‡ä»¶ä¸­åˆ é™¤
    st.session_state.uploaded_files = [f for f in st.session_state.uploaded_files if f['id'] != file_id]

    # æ·»åŠ åˆ°åˆ é™¤çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºæ¢å¤ï¼‰
    deleted_file = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if not deleted_file:
        file_to_delete = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)
        if file_to_delete:
            st.session_state.deleted_files.append({
                **file_to_delete,
                'deleted_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })

    # ä»çŸ¥è¯†åº“ä¸­åˆ é™¤ç›¸å…³ç‰‡æ®µ
    st.session_state.knowledge_base = [kb for kb in st.session_state.knowledge_base if kb['source_id'] != file_id]

    # æ›´æ–°å‘é‡å­˜å‚¨
    update_vector_store()

# æ¢å¤å·²åˆ é™¤æ–‡ä»¶
def restore_file(file_id):
    # ä»åˆ é™¤åˆ—è¡¨ä¸­æ¢å¤
    file_to_restore = next((f for f in st.session_state.deleted_files if f['id'] == file_id), None)
    if file_to_restore:
        st.session_state.uploaded_files.append({
            'id': file_to_restore['id'],
            'name': file_to_restore['name'],
            'type': file_to_restore['type'],
            'content': file_to_restore['content'],
            'upload_time': file_to_restore['upload_time'],
            'tags': file_to_restore['tags'] if 'tags' in file_to_restore else []
        })

        # ä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
        st.session_state.deleted_files = [f for f in st.session_state.deleted_files if f['id'] != file_id]

        # é‡æ–°æ·»åŠ åˆ°çŸ¥è¯†åº“
        chunks = split_into_chunks(file_to_restore['content'])
        for i, chunk in enumerate(chunks):
            st.session_state.knowledge_base.append({
                "source": file_to_restore['name'],
                "source_id": file_to_restore['id'],
                "content": chunk,
                "type": file_to_restore['type'].split("/")[-1]
            })

        # æ›´æ–°å‘é‡å­˜å‚¨
        update_vector_store()
        return True
    return False

# æ ‡è®°æ–‡ä»¶åŠŸèƒ½
def toggle_file_tag(file_id, tag):
    for file in st.session_state.uploaded_files:
        if file['id'] == file_id:
            if 'tags' not in file:
                file['tags'] = []

            if tag in file['tags']:
                file['tags'].remove(tag)
            else:
                file['tags'].append(tag)
            return True
    return False

# æ›´æ–°å‘é‡å­˜å‚¨
def update_vector_store():
    if st.session_state.embedding_model and st.session_state.knowledge_base:
        model = st.session_state.embedding_model
        contents = [kb["content"] for kb in st.session_state.knowledge_base]
        st.session_state.knowledge_vectors = model.encode(contents)

# åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # ç¡®ä¿åµŒå…¥æ¨¡å‹å·²åŠ è½½
        if not st.session_state.embedding_model:
            with st.spinner("åŠ è½½è¯­ä¹‰æ¨¡å‹..."):
                st.session_state.embedding_model = load_embedding_model()

        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è¿‡ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            file_id = generate_file_id(file.getvalue())
            existing_file = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)

            if not existing_file:
                with st.spinner(f"è§£ææ–‡ä»¶: {file.name}..."):
                    content = parse_file(file)

                    if content:
                        st.session_state.uploaded_files.append({
                            "id": file_id,
                            "name": file.name,
                            "type": file.type,
                            "content": content,
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "tags": ["æ–°ä¸Šä¼ "]  # é»˜è®¤æ ‡è®°
                        })

                        # åˆ†å‰²å†…å®¹ä¸ºçŸ¥è¯†ç‰‡æ®µ
                        chunks = st.session_state.text_splitter.split_text(content)
                        
                        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                        st.session_state.vector_db.add_texts(
                            texts=chunks,
                            metadatas=[{
                                "source": file.name,
                                "source_id": file_id,
                                "type": file.type.split("/")[-1],
                                "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            } for _ in chunks]
                        )

                        # ä¿ç•™åˆ°çŸ¥è¯†åº“
                        for i, chunk in enumerate(chunks):
                            st.session_state.knowledge_base.append({
                                "source": file.name,
                                "source_id": file_id,
                                "content": chunk,
                                "type": file.type.split("/")[-1]
                            })

        # æ›´æ–°å‘é‡å­˜å‚¨
        update_vector_store()
        st.rerun()

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
    st.subheader("æ–‡æ¡£ç®¡ç†")
    st.info("ä½¿ç”¨ä»¥ä¸‹è¡¨æ ¼ç®¡ç†æ‚¨çš„æ–‡æ¡£ï¼š")

    files_df = pd.DataFrame([
        {
            "ID": f["id"],
            "æ–‡ä»¶å": f["name"],
            "ç±»å‹": f["type"],
            "å¤§å°": f"{len(f['content']) // 1024} KB",
            "ä¸Šä¼ æ—¶é—´": f["upload_time"],
            "æ ‡è®°": ", ".join(f.get("tags", [])) if "tags" in f else "",
        }
        for f in st.session_state.uploaded_files
    ])

    # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡ä»¶æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if files_df.empty:
        st.info("æš‚æ— æ–‡æ¡£ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")
    else:
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(files_df.set_index('ID'), use_container_width=True)

        # æ–‡ä»¶ç®¡ç†åŠŸèƒ½åŒº
        with st.expander("ğŸ“Œ æ–‡ä»¶æ“ä½œ", expanded=True):
            col1, col2, col3, col4 = st.columns([0.4, 0.2, 0.2, 0.2])

            # æ–‡ä»¶é€‰æ‹©
            file_ids = list(files_df["ID"])
            selected_file_id = col1.selectbox("é€‰æ‹©æ–‡ä»¶", file_ids, format_func=lambda id: 
                files_df.loc[files_df["ID"] == id, "æ–‡ä»¶å"].values[0])

            # æ ‡è®°ç®¡ç†
            tags = ["é‡è¦", "å¾…å®¡æ ¸", "å­˜æ¡£", "å‚è€ƒ"]
            selected_tag = col2.selectbox("æ·»åŠ /ç§»é™¤æ ‡è®°", tags)

            # æŒ‰é’®æ“ä½œ
            if col3.button("åº”ç”¨æ ‡è®°", key="tag_btn", use_container_width=True):
                toggle_file_tag(selected_file_id, selected_tag)
                st.rerun()

            if col4.button("åˆ é™¤æ–‡ä»¶", key="delete_btn", type="primary", use_container_width=True):
                delete_file(selected_file_id)
                st.rerun()

        # å›æ”¶ç«™ç®¡ç†
        if st.session_state.deleted_files:
            with st.expander("ğŸ—‘ï¸ å›æ”¶ç«™ç®¡ç†", expanded=True):
                deleted_df = pd.DataFrame([
                    {
                        "ID": f["id"],
                        "æ–‡ä»¶å": f["name"],
                        "ç±»å‹": f["type"],
                        "åˆ é™¤æ—¶é—´": f["deleted_time"],
                    }
                    for f in st.session_state.deleted_files
                ])

                st.dataframe(deleted_df.set_index('ID'), use_container_width=True)

                restore_id = st.selectbox("é€‰æ‹©è¦æ¢å¤çš„æ–‡ä»¶", deleted_df["ID"], format_func=lambda id: 
                    deleted_df.loc[deleted_df["ID"] == id, "æ–‡ä»¶å"].values[0])

                if st.button("æ¢å¤æ–‡ä»¶", use_container_width=True):
                    if restore_file(restore_id):
                        st.success(f"æ–‡ä»¶å·²æ¢å¤")
                        st.rerun()
                    else:
                        st.error("æ¢å¤æ–‡ä»¶å¤±è´¥")

                if st.button("æ¸…ç©ºå›æ”¶ç«™", type="primary", use_container_width=True):
                    st.session_state.deleted_files = []
                    st.success("å›æ”¶ç«™å·²æ¸…ç©º")
                    st.rerun()

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("ğŸ” æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹", expanded=False):
            if not st.session_state.knowledge_base:
                st.info("çŸ¥è¯†åº“ä¸ºç©º")
            else:
                # åˆ†ç»„æ˜¾ç¤ºæŒ‰æ–‡ä»¶
                source_files = set(kb['source_id'] for kb in st.session_state.knowledge_base)
                for src_id in list(source_files)[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„å†…å®¹
                    source_name = next(
                        kb['source'] for kb in st.session_state.knowledge_base if kb['source_id'] == src_id)
                    st.subheader(f"æ¥æº: {source_name}")

                    # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªç‰‡æ®µ
                    for kb in [k for k in st.session_state.knowledge_base if k['source_id'] == src_id][:3]:
                        with st.expander(f"çŸ¥è¯†ç‰‡æ®µ {kb['content'][:30]}...", expanded=False):
                            st.markdown(kb["content"])
                    st.divider()
                if len(source_files) > 3:
                    st.info(f"å·²æ˜¾ç¤º3ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå…±{len(source_files)}ä¸ªæ–‡ä»¶")

# é—®ç­”ç•Œé¢ï¼ˆç»“åˆè¯­ä¹‰ç†è§£å’ŒDeepSeekï¼‰
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    # æ˜¾ç¤ºå¯¹è¯å†å²
    if st.session_state.conversation:
        for msg in st.session_state.conversation:
            role = "user" if msg.startswith("ç”¨æˆ·:") else "assistant"
            with st.chat_message(role):
                st.write(msg.split(":", 1)[1].strip())

    # ç”¨æˆ·æé—®å¤„ç†
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        # 1. è¯­ä¹‰åˆ†æå¤„ç†
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜è¯­ä¹‰..."):
            semantic_info = semantic_analysis(question)
            intent = semantic_info["intent"]
            entities = semantic_info["entities"]
            question_embedding = semantic_info["embedding"]

        # 2. å‘é‡æ£€ç´¢
        docs = st.session_state.vector_db.similarity_search(question, k=3)

        # 3. æ„å»ºç§‘å­¦é—®ç­”æç¤ºè¯
        context = "\n".join([
            f"ã€æ–‡çŒ® {i + 1}ã€‘{doc.metadata['source']}\n{doc.page_content}\n"
            for i, doc in enumerate(docs)
        ])

        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ï¼š
{context}
é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
1. å›ç­”éœ€å¼•ç”¨æ–‡çŒ®ï¼ˆä¾‹ï¼šã€æ–‡çŒ®1ã€‘ï¼‰
2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§
3. å¦‚æ— ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜
4. é—®é¢˜æ„å›¾ï¼š{intent}
5. å…³é”®å®ä½“ï¼š{', '.join(entities)}
å›ç­”ï¼š"""

        # 4. è°ƒç”¨DeepSeekç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                answer = ask_ai(prompt)
                st.write(answer)
                st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
            with st.expander("ğŸ“š å‚è€ƒæ–‡æ¡£", expanded=False):
                for i, doc in enumerate(docs, 1):
                    st.caption(f"ã€æ–‡çŒ®{i}ã€‘{doc.metadata['source']}")
                    st.text(doc.page_content[:200] + "...")

            # æ˜¾ç¤ºè¯­ä¹‰åˆ†æè¯¦æƒ…
            with st.expander("ğŸ” è¯­ä¹‰åˆ†æè¯¦æƒ…", expanded=False):
                st.json({
                    "é—®é¢˜æ„å›¾": intent,
                    "è¯†åˆ«å®ä½“": entities,
                    "åŒ¹é…ç‰‡æ®µæ•°": len(docs),
                    "æç¤ºè¯": prompt[:500] + "..." if len(prompt) > 500 else prompt
                })

# ä¸»ç•Œé¢
def main():
    st.set_page_config(
        page_title="æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    # æ–°å¢é¢„ä¸‹è½½æ£€æŸ¥
    if "model_downloaded" not in st.session_state:
        with st.spinner("æ­£åœ¨é¢„ä¸‹è½½è¯­ä¹‰æ¨¡å‹ï¼ˆçº¦1.2GBï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦æ—¶é—´ï¼‰..."):
            try:
                # å¼ºåˆ¶æå‰ä¸‹è½½
                model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                st.session_state.model_downloaded = True
                st.session_state.embedding_model = model
            except Exception as e:
                st.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
                return
    
    setup_deepseek()
    init_session()
    st.title("ğŸ“š æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ")
    st.caption("çŸ¥è¯†åº“æ„å»ºã€ç®¡ç†åŠæ™ºèƒ½é—®ç­”å¹³å° | æ”¯æŒæ–‡æ¡£å¤„ç†ä¸è¯­ä¹‰åˆ†æ")

    setup_deepseek()  # åˆå§‹åŒ–DeepSeeké…ç½®
    init_session()

    # åˆ›å»ºä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.header("ğŸ” å¯¼èˆªèœå•")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"], horizontal=True)
        st.divider()

        # ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ
        st.subheader("ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ")
        col1, col2, col3 = st.columns(3)
        col1.metric("çŸ¥è¯†æ–‡æ¡£", len(st.session_state.uploaded_files))
        col2.metric("çŸ¥è¯†ç‰‡æ®µ", len(st.session_state.knowledge_base))
        col3.metric("å›æ”¶ç«™", len(st.session_state.deleted_files))
        st.divider()

        st.info("""
        **ç³»ç»ŸåŠŸèƒ½ï¼š**
        1. çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†
        2. æ–‡æ¡£è§£æä¸å¤„ç†
        3. è¯­ä¹‰åˆ†æä¸ç†è§£
        4. æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        5. æ–‡ä»¶æ ‡è®°ä¸å›æ”¶
        """)

        # æ•°æ®ç®¡ç†é€‰é¡¹
        if st.button("æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True, type="secondary"):
            st.session_state.uploaded_files = []
            st.session_state.knowledge_base = []
            st.session_state.conversation = []
            st.session_state.knowledge_vectors = []
            st.session_state.vector_db = Chroma(
                embedding_function=st.session_state.embedding_model_langchain,
                persist_directory="./chroma_db"
            )
            st.rerun()

    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        qa_interface()

    # è°ƒè¯•ä¿¡æ¯
    with st.expander("ğŸ› ï¸ è°ƒè¯•ä¿¡æ¯", expanded=False):
        st.json({
            "æ–‡ä»¶ä¸Šä¼ æ•°": len(st.session_state.uploaded_files),
            "çŸ¥è¯†ç‰‡æ®µæ•°": len(st.session_state.knowledge_base),
            "å‘é‡å­˜å‚¨æ•°": len(st.session_state.knowledge_vectors) if hasattr(st.session_state.knowledge_vectors, '__len__') else 0,
            "åˆ é™¤æ–‡ä»¶æ•°": len(st.session_state.deleted_files),
            "å¯¹è¯è½®æ¬¡": len(st.session_state.conversation) // 2
        })

if __name__ == "__main__":
    main()