import streamlit as st
import pandas as pd
from datetime import datetime
from Files_Operator import parse_file, generate_file_id, load_embedding_model
from Database_Operator import (
    delete_file,
    restore_file,
    toggle_file_tag,
    semantic_analysis,
    update_vector_store
)
from AI_Respond import ask_ai
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer

# DeepSeek AI æ¥å£é…ç½®ï¼ˆå¦‚éœ€ï¼‰
def setup_deepseek():
    import openai
    # openai.api_key = st.secrets.get("OPENAI_API_KEY", "")
    # openai.base_url = "https://api.gpt.ge/v1/"
    pass

# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")
    # åˆå§‹åŒ– session_state å­—æ®µ
    for key, default in [
        ("embedding_model", None),
        ("uploaded_files", []),
        ("vector_db", None),
        ("text_splitter", None),
        ("knowledge_base", []),
        ("deleted_files", []),
    ]:
        if key not in st.session_state:
            st.session_state[key] = default

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # ç¡®ä¿åµŒå…¥æ¨¡å‹å·²åŠ è½½
        if not st.session_state.embedding_model:
            with st.spinner("åŠ è½½è¯­ä¹‰æ¨¡å‹..."):
                st.session_state.embedding_model = load_embedding_model()

        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ è¿‡ç›¸åŒå†…å®¹çš„æ–‡ä»¶
            file_id = generate_file_id(file.getvalue())
            existing_file = next((f for f in st.session_state.uploaded_files if f['id'] == file_id), None)

            if not existing_file:
                with st.spinner(f"è§£ææ–‡ä»¶: {file.name}..."):
                    content = parse_file(file)

                    if content:
                        st.session_state.uploaded_files.append({
                            "id": file_id,
                            "name": file.name,
                            "type": file.type,
                            "content": content,
                            "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "tags": ["æ–°ä¸Šä¼ "]  # é»˜è®¤æ ‡è®°
                        })

                        # åˆ†å‰²å†…å®¹ä¸ºçŸ¥è¯†ç‰‡æ®µ
                        if st.session_state.text_splitter and hasattr(st.session_state.text_splitter, "split_text"):
                            chunks = st.session_state.text_splitter.split_text(content)
                        else:
                            # å…œåº•ï¼šç®€å•æŒ‰æ®µè½åˆ†å‰²
                            chunks = content.split('\n\n')

                        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                        st.session_state.vector_db.add_texts(
                            texts=chunks,
                            metadatas=[{
                                "source": file.name,
                                "source_id": file_id,
                                "type": file.type.split("/")[-1],
                                "upload_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            } for _ in chunks]
                        )

                        # ä¿ç•™åˆ°çŸ¥è¯†åº“
                        for i, chunk in enumerate(chunks):
                            st.session_state.knowledge_base.append({
                                "source": file.name,
                                "source_id": file_id,
                                "content": chunk,
                                "type": file.type.split("/")[-1]
                            })

        # æ›´æ–°å‘é‡å­˜å‚¨
        update_vector_store()
        st.rerun()

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
    st.subheader("æ–‡æ¡£ç®¡ç†")
    st.info("ä½¿ç”¨ä»¥ä¸‹è¡¨æ ¼ç®¡ç†æ‚¨çš„æ–‡æ¡£ï¼š")

    files_df = pd.DataFrame([
        {
            "ID": f["id"],
            "æ–‡ä»¶å": f["name"],
            "ç±»å‹": f["type"],
            "å¤§å°": f"{len(f['content']) // 1024} KB",
            "ä¸Šä¼ æ—¶é—´": f["upload_time"],
            "æ ‡è®°": ", ".join(f.get("tags", [])) if "tags" in f else "",
        }
        for f in st.session_state.uploaded_files
    ])

    # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡ä»¶æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if files_df.empty:
        st.info("æš‚æ— æ–‡æ¡£ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")
    else:
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(files_df.set_index('ID'), use_container_width=True)

        # æ–‡ä»¶ç®¡ç†åŠŸèƒ½åŒº
        with st.expander("ğŸ“Œ æ–‡ä»¶æ“ä½œ", expanded=True):
            col1, col2, col3, col4 = st.columns([0.4, 0.2, 0.2, 0.2])

            # æ–‡ä»¶é€‰æ‹©
            file_ids = list(files_df["ID"])
            selected_file_id = col1.selectbox("é€‰æ‹©æ–‡ä»¶", file_ids, format_func=lambda id: 
                files_df.loc[files_df["ID"] == id, "æ–‡ä»¶å"].values[0])

            # æ ‡è®°ç®¡ç†
            tags = ["é‡è¦", "å¾…å®¡æ ¸", "å­˜æ¡£", "å‚è€ƒ"]
            selected_tag = col2.selectbox("æ·»åŠ /ç§»é™¤æ ‡è®°", tags)

            # æŒ‰é’®æ“ä½œ
            if col3.button("åº”ç”¨æ ‡è®°", key="tag_btn", use_container_width=True):
                toggle_file_tag(selected_file_id, selected_tag)
                st.rerun()

            if col4.button("åˆ é™¤æ–‡ä»¶", key="delete_btn", type="primary", use_container_width=True):
                delete_file(selected_file_id)
                st.rerun()

        # å›æ”¶ç«™ç®¡ç†
        if st.session_state.deleted_files:
            with st.expander("ğŸ—‘ï¸ å›æ”¶ç«™ç®¡ç†", expanded=True):
                deleted_df = pd.DataFrame([
                    {
                        "ID": f["id"],
                        "æ–‡ä»¶å": f["name"],
                        "ç±»å‹": f["type"],
                        "åˆ é™¤æ—¶é—´": f["deleted_time"],
                    }
                    for f in st.session_state.deleted_files
                ])

                st.dataframe(deleted_df.set_index('ID'), use_container_width=True)

                restore_id = st.selectbox("é€‰æ‹©è¦æ¢å¤çš„æ–‡ä»¶", deleted_df["ID"], format_func=lambda id: 
                    deleted_df.loc[deleted_df["ID"] == id, "æ–‡ä»¶å"].values[0])

                if st.button("æ¢å¤æ–‡ä»¶", use_container_width=True):
                    if restore_file(restore_id):
                        st.success(f"æ–‡ä»¶å·²æ¢å¤")
                        st.rerun()
                    else:
                        st.error("æ¢å¤æ–‡ä»¶å¤±è´¥")

                if st.button("æ¸…ç©ºå›æ”¶ç«™", type="primary", use_container_width=True):
                    st.session_state.deleted_files = []
                    st.success("å›æ”¶ç«™å·²æ¸…ç©º")
                    st.rerun()

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("ğŸ” æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹", expanded=False):
            if not st.session_state.knowledge_base:
                st.info("çŸ¥è¯†åº“ä¸ºç©º")
            else:
                # åˆ†ç»„æ˜¾ç¤ºæŒ‰æ–‡ä»¶
                source_files = set(kb['source_id'] for kb in st.session_state.knowledge_base)
                for src_id in list(source_files)[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„å†…å®¹
                    source_name = next(
                        kb['source'] for kb in st.session_state.knowledge_base if kb['source_id'] == src_id)
                    st.subheader(f"æ¥æº: {source_name}")

                    # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªç‰‡æ®µ
                    for kb in [k for k in st.session_state.knowledge_base if k['source_id'] == src_id][:3]:
                        with st.expander(f"çŸ¥è¯†ç‰‡æ®µ {kb['content'][:30]}...", expanded=False):
                            st.markdown(kb["content"])
                    st.divider()
                if len(source_files) > 3:
                    st.info(f"å·²æ˜¾ç¤º3ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå…±{len(source_files)}ä¸ªæ–‡ä»¶")

# é—®ç­”ç•Œé¢ï¼ˆç»“åˆè¯­ä¹‰ç†è§£å’ŒDeepSeekï¼‰
def qa_interface():
    st.header("ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    # åˆå§‹åŒ–å¯¹è¯å†å²
    if "conversation" not in st.session_state:
        st.session_state.conversation = []

    # æ˜¾ç¤ºå¯¹è¯å†å²
    if st.session_state.conversation:
        for msg in st.session_state.conversation:
            role = "user" if msg.startswith("ç”¨æˆ·:") else "assistant"
            with st.chat_message(role):
                st.write(msg.split(":", 1)[1].strip())

    # ç”¨æˆ·æé—®å¤„ç†
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")

        # 1. è¯­ä¹‰åˆ†æå¤„ç†
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜è¯­ä¹‰..."):
            semantic_info = semantic_analysis(question)
            intent = semantic_info["intent"]
            entities = semantic_info["entities"]
            question_embedding = semantic_info["embedding"]

        # 2. å‘é‡æ£€ç´¢
        docs = st.session_state.vector_db.similarity_search(question, k=3)

        # 3. æ„å»ºç§‘å­¦é—®ç­”æç¤ºè¯
        context = "\n".join([
            f"ã€æ–‡çŒ® {i + 1}ã€‘{doc.metadata['source']}\n{doc.page_content}\n"
            for i, doc in enumerate(docs)
        ])

        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ï¼š
{context}
é—®é¢˜ï¼š{question}
è¦æ±‚ï¼š
1. å›ç­”éœ€å¼•ç”¨æ–‡çŒ®ï¼ˆä¾‹ï¼šã€æ–‡çŒ®1ã€‘ï¼‰
2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§
3. å¦‚æ— ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜
4. é—®é¢˜æ„å›¾ï¼š{intent}
5. å…³é”®å®ä½“ï¼š{', '.join(entities)}
å›ç­”ï¼š"""

        # 4. è°ƒç”¨DeepSeekç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                answer = ask_ai(prompt)
                st.write(answer)
                st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
            with st.expander("ğŸ“š å‚è€ƒæ–‡æ¡£", expanded=False):
                for i, doc in enumerate(docs, 1):
                    st.caption(f"ã€æ–‡çŒ®{i}ã€‘{doc.metadata['source']}")
                    st.text(doc.page_content[:200] + "...")

            # æ˜¾ç¤ºè¯­ä¹‰åˆ†æè¯¦æƒ…
            with st.expander("ğŸ” è¯­ä¹‰åˆ†æè¯¦æƒ…", expanded=False):
                st.json({
                    "é—®é¢˜æ„å›¾": intent,
                    "è¯†åˆ«å®ä½“": entities,
                    "åŒ¹é…ç‰‡æ®µæ•°": len(docs),
                    "æç¤ºè¯": prompt[:500] + "..." if len(prompt) > 500 else prompt
                })

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session():
    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = []
    if "knowledge_base" not in st.session_state:
        st.session_state.knowledge_base = []
    if "deleted_files" not in st.session_state:
        st.session_state.deleted_files = []
    if "embedding_model" not in st.session_state:
        st.session_state.embedding_model = None
    if "knowledge_vectors" not in st.session_state:
        st.session_state.knowledge_vectors = []
    if "vector_db" not in st.session_state:
        st.session_state.embedding_model_langchain = HuggingFaceEmbeddings(
            model_name="GanymedeNil/text2vec-large-chinese",
            model_kwargs={'device': 'cpu'}
        )
        st.session_state.vector_db = Chroma(
            embedding_function=st.session_state.embedding_model_langchain,
            persist_directory="./chroma_db"
        )
    if "text_splitter" not in st.session_state:
        st.session_state.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )

def main():
    st.set_page_config(
        page_title="æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ“š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    # æ–°å¢é¢„ä¸‹è½½æ£€æŸ¥
    if "model_downloaded" not in st.session_state:
        with st.spinner("æ­£åœ¨é¢„ä¸‹è½½è¯­ä¹‰æ¨¡å‹ï¼ˆçº¦1.2GBï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦æ—¶é—´ï¼‰..."):
            try:
                # å¼ºåˆ¶æå‰ä¸‹è½½
                model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                st.session_state.model_downloaded = True
                st.session_state.embedding_model = model
            except Exception as e:
                st.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
                return

    setup_deepseek()
    init_session()
    st.title("ğŸ“š æ™ºèƒ½æ–‡çŒ®é—®ç­”ç³»ç»Ÿ")
    st.caption("çŸ¥è¯†åº“æ„å»ºã€ç®¡ç†åŠæ™ºèƒ½é—®ç­”å¹³å° | æ”¯æŒæ–‡æ¡£å¤„ç†ä¸è¯­ä¹‰åˆ†æ")

    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.subheader("å¯¼èˆª")
        page = st.radio("é€‰æ‹©åŠŸèƒ½", ["çŸ¥è¯†åº“ç®¡ç†", "æ™ºèƒ½é—®ç­”"])

    # æ˜¾ç¤ºå¯¹åº”çš„é¡µé¢
    if page == "çŸ¥è¯†åº“ç®¡ç†":
        knowledge_base_section()
    else:
        # ä¿è¯å¯¹è¯å†å²åˆå§‹åŒ–
        if "conversation" not in st.session_state:
            st.session_state.conversation = []
        qa_interface()

if __name__ == "__main__":
    main()