#ui
import streamlit as st
import pandas as pd
from datetime import datetime
from file_registry import FileRegistry
from knowledge_base_manager import (
    delete_file,
    restore_file,
    toggle_file_tag,
    semantic_analysis,
    add_file_to_knowledge_base
)
from ai_service import ask_ai
import vector_store as db_op

# çŸ¥è¯†åº“ç®¡ç†ç•Œé¢
def knowledge_base_section():
    st.header("ğŸ“š çŸ¥è¯†åº“æ„å»ºä¸ç®¡ç†")

    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£ (æ”¯æŒå¤šç§æ ¼å¼)",
        type=["txt", "pdf", "docx", "pptx", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        # å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
        for file in uploaded_files:
            add_file_to_knowledge_base(file)
        
        # Rerun to reflect changes immediately
        st.rerun()

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
    st.subheader("æ–‡æ¡£ç®¡ç†")
    st.info("ä½¿ç”¨ä»¥ä¸‹è¡¨æ ¼ç®¡ç†æ‚¨çš„æ–‡æ¡£ï¼š")

    files_df = pd.DataFrame([
        {
            "ID": f["id"],
            "æ–‡ä»¶å": f["name"],
            "ç±»å‹": f["type"],
            "å¤§å°": f"{len(f['content']) // 1024} KB" if f.get('content') else "æœªçŸ¥",
            "ä¸Šä¼ æ—¶é—´": f["upload_time"],
            "æ ‡è®°": ", ".join(f.get("tags", [])) if "tags" in f else "",
        }
        for f in st.session_state.uploaded_files
    ])

    # å¦‚æœæ²¡æœ‰ä»»ä½•æ–‡ä»¶æ˜¾ç¤ºæç¤ºä¿¡æ¯
    if files_df.empty:
        st.info("æš‚æ— æ–‡æ¡£ï¼Œè¯·ä¸Šä¼ æ–‡æ¡£")
    else:
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(files_df.set_index('ID'), use_container_width=True)

        # æ–‡ä»¶ç®¡ç†åŠŸèƒ½åŒº
        with st.expander("ğŸ“Œ æ–‡ä»¶æ“ä½œ", expanded=True):
            col1, col2, col3, col4 = st.columns([0.4, 0.2, 0.2, 0.2])

            # æ–‡ä»¶é€‰æ‹©
            file_ids = list(files_df["ID"])
            selected_file_id = col1.selectbox("é€‰æ‹©æ–‡ä»¶", file_ids, format_func=lambda id: 
                files_df.loc[files_df["ID"] == id, "æ–‡ä»¶å"].values[0])

            # æ ‡è®°ç®¡ç†
            tags = ["é‡è¦", "å¾…å®¡æ ¸", "å­˜æ¡£", "å‚è€ƒ"]
            selected_tag = col2.selectbox("æ·»åŠ /ç§»é™¤æ ‡è®°", tags)

            # æŒ‰é’®æ“ä½œ
            if col3.button("åº”ç”¨æ ‡è®°", key="tag_btn", use_container_width=True):
                toggle_file_tag(selected_file_id, selected_tag)
                st.rerun()

            if col4.button("åˆ é™¤æ–‡ä»¶", key="delete_btn", type="primary", use_container_width=True):
                delete_file(selected_file_id)
                st.rerun()

        # å›æ”¶ç«™ç®¡ç†
        if st.session_state.deleted_files:
            with st.expander("ğŸ—‘ï¸ å›æ”¶ç«™ç®¡ç†", expanded=True):
                deleted_df = pd.DataFrame([
                    {
                        "ID": f["id"],
                        "æ–‡ä»¶å": f["name"],
                        "ç±»å‹": f["type"],
                        "åˆ é™¤æ—¶é—´": f["deleted_time"],
                    }
                    for f in st.session_state.deleted_files
                ])

                st.dataframe(deleted_df.set_index('ID'), use_container_width=True)

                restore_id = st.selectbox("é€‰æ‹©è¦æ¢å¤çš„æ–‡ä»¶", deleted_df["ID"], format_func=lambda id: 
                    deleted_df.loc[deleted_df["ID"] == id, "æ–‡ä»¶å"].values[0])

                if st.button("æ¢å¤æ–‡ä»¶", use_container_width=True):
                    if restore_file(restore_id):
                        st.success(f"æ–‡ä»¶å·²æ¢å¤")
                        st.rerun()
                    else:
                        st.error("æ¢å¤æ–‡ä»¶å¤±è´¥")

                if st.button("æ¸…ç©ºå›æ”¶ç«™", type="primary", use_container_width=True):
                    st.session_state.deleted_files = []
                    st.success("å›æ”¶ç«™å·²æ¸…ç©º")
                    st.rerun()

        # æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹
        with st.expander("ğŸ” æŸ¥çœ‹çŸ¥è¯†åº“å†…å®¹", expanded=False):
            if not st.session_state.knowledge_base:
                st.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚")
                return
            # åˆ†ç»„æ˜¾ç¤ºæŒ‰æ–‡ä»¶
            source_files = set(kb['source_id'] for kb in st.session_state.knowledge_base)
            for src_id in list(source_files)[:3]:  # æœ€å¤šæ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶çš„å†…å®¹
                source_name = next(
                    kb['source'] for kb in st.session_state.knowledge_base if kb['source_id'] == src_id)
                st.subheader(f"æ¥æº: {source_name}")

                # æ˜¾ç¤ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªç‰‡æ®µ
                for kb in [k for k in st.session_state.knowledge_base if k['source_id'] == src_id][:3]:
                    with st.expander(f"çŸ¥è¯†ç‰‡æ®µ {kb['content'][:30]}...", expanded=False):
                        st.markdown(kb["content"])
                st.divider()
            if len(source_files) > 3:
                st.info(f"å·²æ˜¾ç¤º3ä¸ªæ–‡ä»¶çš„å†…å®¹ï¼Œå…±{len(source_files)}ä¸ªæ–‡ä»¶")

# é—®ç­”ç•Œé¢ï¼ˆç»“åˆè¯­ä¹‰ç†è§£å’ŒDeepSeekï¼‰
def qa_interface():
    st.header("ğŸ’¬ğŸ’¬ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    # æ˜¾ç¤ºå¯¹è¯å†å²
    if st.session_state.conversation:
        for msg in st.session_state.conversation:
            role = "user" if msg.startswith("ç”¨æˆ·:") else "assistant"
            with st.chat_message(role):
                st.write(msg.split(":", 1)[1].strip())

    # ç”¨æˆ·æé—®å¤„ç†
    if question := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        st.session_state.conversation.append(f"ç”¨æˆ·: {question}")
        
        # 1. ä¸Šä¸‹æ–‡åˆ†æ - æ£€æŸ¥æ˜¯å¦ä¸ä¸Šä¸€ä¸ªé—®é¢˜ç›¸å…³
        context_analysis = ""
        if len(st.session_state.conversation) >= 2:
            last_question = st.session_state.conversation[-2]
            if last_question.startswith("ç”¨æˆ·:"):
                last_question = last_question.split(":", 1)[1].strip()
                
                # è®¡ç®—å½“å‰é—®é¢˜ä¸ä¸Šä¸€ä¸ªé—®é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦
                model = st.session_state.embedding_model
                last_embedding = model.encode([last_question])[0]
                current_embedding = model.encode([question])[0]
                
                # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                from numpy import dot
                from numpy.linalg import norm
                similarity = dot(last_embedding, current_embedding)/(norm(last_embedding)*norm(current_embedding))
                
                if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    context_analysis = f"\næ³¨æ„ï¼šè¿™ä¸ªé—®é¢˜ä¸ä¸Šä¸€ä¸ªé—®é¢˜é«˜åº¦ç›¸å…³ï¼ˆç›¸ä¼¼åº¦{similarity:.2f}ï¼‰ï¼Œè¯·è€ƒè™‘ä¸Šä¸‹æ–‡å›ç­”ã€‚"
                    # è·å–ä¸Šä¸€ä¸ªé—®é¢˜çš„å›ç­”
                    last_answer = ""
                    if len(st.session_state.conversation) >= 3:
                        last_answer_msg = st.session_state.conversation[-3]
                        if last_answer_msg.startswith("ç³»ç»Ÿ:"):
                            last_answer = last_answer_msg.split(":", 1)[1].strip()
                    
                    context_analysis += f"\nä¸Šä¸€ä¸ªé—®é¢˜: {last_question}\nä¸Šä¸€ä¸ªå›ç­”: {last_answer}"

        # 2. è¯­ä¹‰åˆ†æå¤„ç†
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜è¯­ä¹‰..."):
            semantic_info = semantic_analysis(question)
            intent = semantic_info["intent"]
            entities = semantic_info["entities"]
            question_embedding = semantic_info["embedding"]

        # 3. å‘é‡æ£€ç´¢
        docs = db_op.search_db(question, k=3)

        # 4. æ„å»ºç§‘å­¦é—®ç­”æç¤ºè¯ï¼ˆå¢å¼ºä¸Šä¸‹æ–‡ï¼‰
        context = "\n".join([
            f"ã€æ–‡çŒ® {i + 1}ã€‘{doc.metadata['source']}\n{doc.page_content}\n"
            for i, doc in enumerate(docs)
        ])

        # å…ˆå¤„ç†å¯¹è¯å†å²
        history = '\n'.join([msg for msg in st.session_state.conversation[-6:] if not msg.startswith('ç³»ç»Ÿ:')])

        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ï¼š
{context}
é—®é¢˜ï¼š{question}
{context_analysis}
è¦æ±‚ï¼š
1. å›ç­”éœ€å¼•ç”¨æ–‡çŒ®ï¼ˆä¾‹ï¼šã€æ–‡çŒ®1ã€‘ï¼‰
2. ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§
3. å¦‚æ— ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜
4. é—®é¢˜æ„å›¾ï¼š{intent}
5. å…³é”®å®ä½“ï¼š{', '.join(entities)}
6. è€ƒè™‘ä»¥ä¸‹å¯¹è¯å†å²ï¼š
{history}
å›ç­”ï¼š"""

        # 5. è°ƒç”¨DeepSeekç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
                answer = ask_ai(prompt)
                st.write(answer)
                st.session_state.conversation.append(f"ç³»ç»Ÿ: {answer}")

            # æ˜¾ç¤ºå‚è€ƒæ–‡çŒ®
            with st.expander("ğŸ“šğŸ“š å‚è€ƒæ–‡æ¡£", expanded=False):
                for i, doc in enumerate(docs, 1):
                    source = getattr(doc, "metadata", {}).get("source", getattr(doc, "source", f"æ–‡æ¡£{i}"))
                    content = getattr(doc, "page_content", str(doc))[:200] + "..."
                    st.caption(f"ã€æ–‡çŒ®{i}ã€‘{source}")
                    st.text(content)

            # æ˜¾ç¤ºè¯­ä¹‰åˆ†æè¯¦æƒ…
            with st.expander("ğŸ”ğŸ” è¯­ä¹‰åˆ†æè¯¦æƒ…", expanded=False):
                st.json({
                    "é—®é¢˜æ„å›¾": intent,
                    "è¯†åˆ«å®ä½“": entities,
                    "åŒ¹é…ç‰‡æ®µæ•°": len(docs),
                    "ä¸Šä¸‹æ–‡å…³è”åº¦": f"{similarity:.2f}" if 'similarity' in locals() else "æ— ",
                    "æç¤ºè¯": prompt[:500] + "..." if len(prompt) > 500 else prompt
                })

        if not st.session_state.get("embedding_model"):
            st.error("åµŒå…¥æ¨¡å‹æœªåŠ è½½ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚")
            return

        if not question or not question.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ã€‚")
            return